{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhareesha/MLAI/blob/main/LLM/finetuned/DataSynthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIJf6FI9ufTw",
        "outputId": "fdec01c8-3e25-4b15-b658-ae10c0056d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Import the `re` module"
      ],
      "metadata": {
        "id": "Hh6vqxc9lZOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcGznvWO1Rhz",
        "outputId": "d1069d7b-9a2d-46e0-96d1-34e0769b2fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-27.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-27.4.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-27.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzT6HQW8w7Dv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGIaMVsGw7r3"
      },
      "outputs": [],
      "source": [
        "# Define the DDLs\n",
        "ddl = \"\"\"\n",
        "CREATE TABLE customers (\n",
        "  id INT PRIMARY KEY,\n",
        "  name VARCHAR(255),\n",
        "  email VARCHAR(255)\n",
        ");\n",
        "\n",
        "CREATE TABLE orders (\n",
        "  id INT PRIMARY KEY,\n",
        "  customer_id INT,\n",
        "  order_date DATE,\n",
        "  FOREIGN KEY (customer_id) REFERENCES customers(id)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Define the sample data\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 1, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2, 'order_date': '2022-01-15'}\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19K6-eQIxEeY",
        "outputId": "d025630e-a088-4ffd-c6c5-961c34ce2361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id      name                 email  customer_id  order_date\n",
            "0   1  John Doe  john.doe@example.com          NaN         NaN\n",
            "1   2  Jane Doe  jane.doe@example.com          NaN         NaN\n",
            "2   1       NaN                   NaN          1.0  2022-01-01\n",
            "3   2       NaN                   NaN          2.0  2022-01-15\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame for the sample data\n",
        "df = pd.DataFrame(sample_data['customers'] + sample_data['orders'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L_xNObfxE_X"
      },
      "outputs": [],
      "source": [
        "# Define the sequence-to-sequence model\n",
        "model_name = 't5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umoayx3yxSQI"
      },
      "outputs": [],
      "source": [
        "# Define the custom dataset class\n",
        "class DDLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def something(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        print(\"Printing ROW\")\n",
        "        print(row)\n",
        "        print(row['table_name'])\n",
        "        input_text = f\"CREATE TABLE {row['table_name']} ({row['column_name']} {row['data_type']})\"\n",
        "        output_text = row['sample_data']\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt')\n",
        "        labels = self.tokenizer(output_text, return_tensors='pt')\n",
        "        return {'input_ids': inputs['input_ids'].flatten(), 'labels': labels['input_ids'].flatten()}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        # print(\"Printing ROW\")\n",
        "        # print(row)\n",
        "        # Determine the table name based on the index\n",
        "        if idx < len(sample_data['customers']):\n",
        "            table_name = 'customers'\n",
        "            column_name = 'id'\n",
        "            data_type = 'INT'\n",
        "            input_text = f\"CREATE TABLE {table_name} ({column_name} {data_type})\"\n",
        "            output_text = f\"INSERT INTO {table_name} ({column_name}) VALUES ({row['id']})\"\n",
        "        else:\n",
        "            table_name = 'orders'\n",
        "            column_name = 'id'\n",
        "            data_type = 'INT'\n",
        "            input_text = f\"CREATE TABLE {table_name} ({column_name} {data_type})\"\n",
        "            output_text = f\"INSERT INTO {table_name} ({column_name}) VALUES ({row['id']})\"\n",
        "\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt')\n",
        "        labels = self.tokenizer(output_text, return_tensors='pt')\n",
        "        return {'input_ids': inputs['input_ids'].flatten(), 'labels': labels['input_ids'].flatten()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEuqmcGWxV3X"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and data loader\n",
        "dataset = DDLDataset(df, tokenizer)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zez7O1P2xZo1",
        "outputId": "20c2a990-fec1-4e7e-a54f-d9a12c5e5c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.685617446899414\n",
            "Epoch 2, Loss: 4.09550666809082\n",
            "Epoch 3, Loss: 3.8358912467956543\n",
            "Epoch 4, Loss: 3.6946582794189453\n",
            "Epoch 5, Loss: 3.5764377117156982\n",
            "Epoch 6, Loss: 3.4286916255950928\n",
            "Epoch 7, Loss: 3.4149281978607178\n",
            "Epoch 8, Loss: 3.5131187438964844\n",
            "Epoch 9, Loss: 3.45695424079895\n",
            "Epoch 10, Loss: 3.0852248668670654\n",
            "Epoch 11, Loss: 3.076669454574585\n",
            "Epoch 12, Loss: 2.892866849899292\n",
            "Epoch 13, Loss: 3.081608295440674\n",
            "Epoch 14, Loss: 2.932990074157715\n",
            "Epoch 15, Loss: 2.8433403968811035\n",
            "Epoch 16, Loss: 2.6560235023498535\n",
            "Epoch 17, Loss: 2.620180606842041\n",
            "Epoch 18, Loss: 2.47871732711792\n",
            "Epoch 19, Loss: 2.657715320587158\n",
            "Epoch 20, Loss: 2.4728803634643555\n",
            "Epoch 21, Loss: 2.499300003051758\n",
            "Epoch 22, Loss: 2.3772404193878174\n",
            "Epoch 23, Loss: 2.2535347938537598\n",
            "Epoch 24, Loss: 2.220264434814453\n",
            "Epoch 25, Loss: 2.2212698459625244\n",
            "Epoch 26, Loss: 2.204831123352051\n",
            "Epoch 27, Loss: 2.1427695751190186\n",
            "Epoch 28, Loss: 2.1274654865264893\n",
            "Epoch 29, Loss: 2.4987714290618896\n",
            "Epoch 30, Loss: 2.6240739822387695\n",
            "Epoch 31, Loss: 2.0532798767089844\n",
            "Epoch 32, Loss: 1.8120644092559814\n",
            "Epoch 33, Loss: 1.76407790184021\n",
            "Epoch 34, Loss: 1.900373935699463\n",
            "Epoch 35, Loss: 1.8742811679840088\n",
            "Epoch 36, Loss: 1.7646371126174927\n",
            "Epoch 37, Loss: 1.6879240274429321\n",
            "Epoch 38, Loss: 1.6438655853271484\n",
            "Epoch 39, Loss: 1.7561670541763306\n",
            "Epoch 40, Loss: 1.5706225633621216\n",
            "Epoch 41, Loss: 1.7212461233139038\n",
            "Epoch 42, Loss: 1.5239529609680176\n",
            "Epoch 43, Loss: 1.6179983615875244\n",
            "Epoch 44, Loss: 1.424320101737976\n",
            "Epoch 45, Loss: 1.506803035736084\n",
            "Epoch 46, Loss: 1.3738734722137451\n",
            "Epoch 47, Loss: 1.338546633720398\n",
            "Epoch 48, Loss: 1.4136197566986084\n",
            "Epoch 49, Loss: 1.4104392528533936\n",
            "Epoch 50, Loss: 1.2940430641174316\n",
            "Epoch 51, Loss: 1.3776534795761108\n",
            "Epoch 52, Loss: 1.2726633548736572\n",
            "Epoch 53, Loss: 1.2643461227416992\n",
            "Epoch 54, Loss: 1.318589448928833\n",
            "Epoch 55, Loss: 1.1143070459365845\n",
            "Epoch 56, Loss: 1.1857266426086426\n",
            "Epoch 57, Loss: 1.1676044464111328\n",
            "Epoch 58, Loss: 1.1269724369049072\n",
            "Epoch 59, Loss: 1.1399476528167725\n",
            "Epoch 60, Loss: 1.071828007698059\n",
            "Epoch 61, Loss: 1.1071311235427856\n",
            "Epoch 62, Loss: 0.9966585636138916\n",
            "Epoch 63, Loss: 1.0371906757354736\n",
            "Epoch 64, Loss: 0.9069271683692932\n",
            "Epoch 65, Loss: 0.9891522526741028\n",
            "Epoch 66, Loss: 1.015772819519043\n",
            "Epoch 67, Loss: 0.8390098214149475\n",
            "Epoch 68, Loss: 0.7520883083343506\n",
            "Epoch 69, Loss: 0.8105220794677734\n",
            "Epoch 70, Loss: 0.9119735360145569\n",
            "Epoch 71, Loss: 0.8478676080703735\n",
            "Epoch 72, Loss: 0.7393307089805603\n",
            "Epoch 73, Loss: 0.8444570302963257\n",
            "Epoch 74, Loss: 0.7928553819656372\n",
            "Epoch 75, Loss: 0.7260040044784546\n",
            "Epoch 76, Loss: 0.6976019144058228\n",
            "Epoch 77, Loss: 0.6078672409057617\n",
            "Epoch 78, Loss: 0.6243420243263245\n",
            "Epoch 79, Loss: 0.8518529534339905\n",
            "Epoch 80, Loss: 0.5898386836051941\n",
            "Epoch 81, Loss: 0.5942803025245667\n",
            "Epoch 82, Loss: 0.6471138000488281\n",
            "Epoch 83, Loss: 0.5806493759155273\n",
            "Epoch 84, Loss: 0.5924093723297119\n",
            "Epoch 85, Loss: 0.5896059274673462\n",
            "Epoch 86, Loss: 0.5562841296195984\n",
            "Epoch 87, Loss: 0.5258883237838745\n",
            "Epoch 88, Loss: 0.5755394697189331\n",
            "Epoch 89, Loss: 0.543883204460144\n",
            "Epoch 90, Loss: 0.4619366228580475\n",
            "Epoch 91, Loss: 0.5190640091896057\n",
            "Epoch 92, Loss: 0.49193742871284485\n",
            "Epoch 93, Loss: 0.4435867369174957\n",
            "Epoch 94, Loss: 0.493852436542511\n",
            "Epoch 95, Loss: 0.47175493836402893\n",
            "Epoch 96, Loss: 0.41677936911582947\n",
            "Epoch 97, Loss: 0.3755970001220703\n",
            "Epoch 98, Loss: 0.4295122027397156\n",
            "Epoch 99, Loss: 0.386772483587265\n",
            "Epoch 100, Loss: 0.37369683384895325\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss() # Create an instance of LossFunction\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        # print(type(outputs))\n",
        "        # loss = criterion(outputs, labels) # Needs to be a Tensor.\n",
        "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
        "    model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OetzoQNzxfyB"
      },
      "outputs": [],
      "source": [
        "# Use the trained model to generate sample data\n",
        "def generate_sample_data_1(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "    print(inputs)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "    print(outputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O53-yl-H1Kfp"
      },
      "outputs": [],
      "source": [
        "import faker\n",
        "\n",
        "def generate_sample_data(input_text):\n",
        "    fake = faker.Faker()\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the table name and column names from the input text\n",
        "    table_name = input_text.split('(')[0].split()[-1]\n",
        "    column_names = input_text.split('(')[1].split(')')[0].split(',')\n",
        "\n",
        "    # Generate sample data using faker\n",
        "    sample_data = []\n",
        "    for _ in range(10):  # Generate 10 rows of sample data\n",
        "        row = {}\n",
        "        for column_name in column_names:\n",
        "            column_name = column_name.strip()\n",
        "            if 'id' in column_name:\n",
        "                row[column_name] = fake.random_int()\n",
        "            elif 'name' in column_name:\n",
        "                row[column_name] = fake.name()\n",
        "            elif 'DATE' in column_name:\n",
        "                row[column_name] = fake.date()\n",
        "            elif 'email' in column_name:\n",
        "                row[column_name] = fake.email()\n",
        "        sample_data.append(row)\n",
        "\n",
        "    return output_text, sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gulv9cVAxgdB",
        "outputId": "a2f9221d-b9c9-4709-807b-d2365a9a8e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('INSERT INTO customers (id, name VARCHAR(255)), email VARCHAR(255))', [{'id INT PRIMARY KEY': 2807, 'name VARCHAR': 'Julie Rodgers'}, {'id INT PRIMARY KEY': 3289, 'name VARCHAR': 'Daniel Johnson'}, {'id INT PRIMARY KEY': 3964, 'name VARCHAR': 'Tiffany Jennings'}, {'id INT PRIMARY KEY': 8647, 'name VARCHAR': 'Lauren Ward DDS'}, {'id INT PRIMARY KEY': 8338, 'name VARCHAR': 'Timothy Patterson'}, {'id INT PRIMARY KEY': 5167, 'name VARCHAR': 'Adam Young'}, {'id INT PRIMARY KEY': 3197, 'name VARCHAR': 'Autumn Underwood'}, {'id INT PRIMARY KEY': 2918, 'name VARCHAR': 'David Johnson'}, {'id INT PRIMARY KEY': 9459, 'name VARCHAR': 'Diane Stewart'}, {'id INT PRIMARY KEY': 123, 'name VARCHAR': 'Diane Reynolds'}])\n"
          ]
        }
      ],
      "source": [
        "input_text = \"CREATE TABLE customers (id INT PRIMARY KEY, name VARCHAR(255), email VARCHAR(255))\"\n",
        "sample_data = generate_sample_data(input_text)\n",
        "print(sample_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qgQXjEEwd_g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC29Ehexxq1u"
      },
      "source": [
        "```\n",
        "# This is formatted as code\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the DDLs\n",
        "ddl = \"\"\"\n",
        "CREATE TABLE customers (\n",
        "  id INT PRIMARY KEY,\n",
        "  name VARCHAR(255),\n",
        "  email VARCHAR(255)\n",
        ");\n",
        "\n",
        "CREATE TABLE orders (\n",
        "  id INT PRIMARY KEY,\n",
        "  customer_id INT,\n",
        "  order_date DATE,\n",
        "  FOREIGN KEY (customer_id) REFERENCES customers(id)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Define the sample data\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 1, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2, 'order_date': '2022-01-15'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame for the sample data\n",
        "df = pd.DataFrame(sample_data['customers'] + sample_data['orders'])\n",
        "print(df)\n",
        "# Define the sequence-to-sequence model\n",
        "model_name = 't5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the custom dataset class\n",
        "class DDLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def something(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        print(\"Printing ROW\")\n",
        "        print(row)\n",
        "        print(row['table_name'])\n",
        "        input_text = f\"CREATE TABLE {row['table_name']} ({row['column_name']} {row['data_type']})\"\n",
        "        output_text = row['sample_data']\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt')\n",
        "        labels = self.tokenizer(output_text, return_tensors='pt')\n",
        "        return {'input_ids': inputs['input_ids'].flatten(), 'labels': labels['input_ids'].flatten()}\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        print(\"Printing ROW\")\n",
        "        print(row)        \n",
        "        # Determine the table name based on the index\n",
        "        if idx < len(sample_data['customers']):\n",
        "            table_name = 'customers'\n",
        "            column_name = 'id'\n",
        "            data_type = 'INT'\n",
        "            input_text = f\"CREATE TABLE {table_name} ({column_name} {data_type})\"\n",
        "            output_text = f\"INSERT INTO {table_name} ({column_name}) VALUES ({row['id']})\"\n",
        "        else:\n",
        "            table_name = 'orders'\n",
        "            column_name = 'id'\n",
        "            data_type = 'INT'\n",
        "            input_text = f\"CREATE TABLE {table_name} ({column_name} {data_type})\"\n",
        "            output_text = f\"INSERT INTO {table_name} ({column_name}) VALUES ({row['id']})\"\n",
        "    \n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt')\n",
        "        labels = self.tokenizer(output_text, return_tensors='pt')\n",
        "        return {'input_ids': inputs['input_ids'].flatten(), 'labels': labels['input_ids'].flatten()}\n",
        "\n",
        "# Create the dataset and data loader\n",
        "dataset = DDLDataset(df, tokenizer)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss() # Create an instance of LossFunction\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        print(type(outputs))\n",
        "        # loss = criterion(outputs, labels) # Needs to be a Tensor.\n",
        "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
        "    model.eval()\n",
        "\n",
        "# Use the trained model to generate sample data\n",
        "def generate_sample_data(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=2000)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "input_text = \"CREATE TABLE customers (id INT PRIMARY KEY, name VARCHAR(255), email VARCHAR(255))\"\n",
        "sample_data = generate_sample_data(input_text)\n",
        "print(sample_data)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW2N8VI_6-o5",
        "outputId": "05d5ac6c-3dbe-4c4e-a37e-dade8d60dcf8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.2246330976486206\n",
            "Epoch 2, Loss: 1.4954220652580261\n",
            "Epoch 3, Loss: 2.4478172063827515\n",
            "Epoch 4, Loss: 2.1031181812286377\n",
            "Epoch 5, Loss: 0.7138573229312897\n",
            "Epoch 6, Loss: 0.6366875469684601\n",
            "Epoch 7, Loss: 1.3573296070098877\n",
            "Epoch 8, Loss: 1.1255963444709778\n",
            "Epoch 9, Loss: 0.4683581292629242\n",
            "Epoch 10, Loss: 0.46456393599510193\n",
            "Epoch 11, Loss: 0.9048112630844116\n",
            "Epoch 12, Loss: 0.7713034152984619\n",
            "Epoch 13, Loss: 0.5606061071157455\n",
            "Epoch 14, Loss: 0.5143173635005951\n",
            "Epoch 15, Loss: 0.39989039301872253\n",
            "Epoch 16, Loss: 0.19277561455965042\n",
            "Epoch 17, Loss: 0.23095811158418655\n",
            "Epoch 18, Loss: 0.10581569001078606\n",
            "Epoch 19, Loss: 0.12392636016011238\n",
            "Epoch 20, Loss: 0.0988464504480362\n",
            "Epoch 21, Loss: 0.112318255007267\n",
            "Epoch 22, Loss: 0.11788876354694366\n",
            "Epoch 23, Loss: 0.11346694827079773\n",
            "Epoch 24, Loss: 0.23099640756845474\n",
            "Epoch 25, Loss: 0.16534792259335518\n",
            "Epoch 26, Loss: 0.09857083112001419\n",
            "Epoch 27, Loss: 0.07259278744459152\n",
            "Epoch 28, Loss: 0.0720802191644907\n",
            "Epoch 29, Loss: 0.13218780979514122\n",
            "Epoch 30, Loss: 0.09234952554106712\n",
            "Epoch 31, Loss: 0.15638630837202072\n",
            "Epoch 32, Loss: 0.07257186993956566\n",
            "Epoch 33, Loss: 0.07315575703978539\n",
            "Epoch 34, Loss: 0.0693419873714447\n",
            "Epoch 35, Loss: 0.1102842278778553\n",
            "Epoch 36, Loss: 0.07290837913751602\n",
            "Epoch 37, Loss: 0.0853019654750824\n",
            "Epoch 38, Loss: 0.21814099326729774\n",
            "Epoch 39, Loss: 0.07697975635528564\n",
            "Epoch 40, Loss: 0.08204768225550652\n",
            "Epoch 41, Loss: 0.08708040416240692\n",
            "Epoch 42, Loss: 0.17622071504592896\n",
            "Epoch 43, Loss: 0.07385767437517643\n",
            "Epoch 44, Loss: 0.10857719369232655\n",
            "Epoch 45, Loss: 0.06493675336241722\n",
            "Epoch 46, Loss: 0.08017724566161633\n",
            "Epoch 47, Loss: 0.055633947253227234\n",
            "Epoch 48, Loss: 0.06352056935429573\n",
            "Epoch 49, Loss: 0.0881001316010952\n",
            "Epoch 50, Loss: 0.07950871624052525\n",
            "Epoch 51, Loss: 0.09143716469407082\n",
            "Epoch 52, Loss: 0.08230076171457767\n",
            "Epoch 53, Loss: 0.08757523819804192\n",
            "Epoch 54, Loss: 0.08580605685710907\n",
            "Epoch 55, Loss: 0.0840672217309475\n",
            "Epoch 56, Loss: 0.057226572185754776\n",
            "Epoch 57, Loss: 0.04575653560459614\n",
            "Epoch 58, Loss: 0.09971605986356735\n",
            "Epoch 59, Loss: 0.0947895385324955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60, Loss: 0.1109996810555458\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'nan', 'nan');\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the sample data\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 1, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2, 'order_date': '2022-01-15'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrames for the sample data\n",
        "customers_df = pd.DataFrame(sample_data['customers'])\n",
        "orders_df = pd.DataFrame(sample_data['orders'])\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'  # You can use 'gpt2', 'gpt2-medium', 'gpt2-large', etc.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Fine-tune the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token for GPT-2\n",
        "\n",
        "# Custom dataset class\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Generate input text based on the table data\n",
        "        if 'name' in row and 'email' in row:\n",
        "            input_text = f\"INSERT INTO customers (id, name, email) VALUES ({row['id']}, '{row['name']}', '{row['email']}');\"\n",
        "        else:\n",
        "            input_text = f\"INSERT INTO orders (id, customer_id, order_date) VALUES ({row['id']}, {row['customer_id']}, '{row['order_date']}');\"\n",
        "\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
        "        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze()}\n",
        "\n",
        "# Create the dataset and data loader\n",
        "combined_df = pd.concat([customers_df, orders_df], ignore_index=True)\n",
        "dataset = SQLDataset(combined_df, tokenizer)\n",
        "\n",
        "# Create a collate function to handle padding dynamically\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # Pad sequences dynamically\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(60):  # You can increase the number of epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
        "\n",
        "# Use the trained model to generate sample data\n",
        "def generate_sql_data(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example SQL generation\n",
        "input_prompt = \"INSERT INTO customers (id, name, email) VALUES\"\n",
        "sample_sql = generate_sql_data(input_prompt)\n",
        "print(sample_sql)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmcaeGbeP5zX"
      },
      "source": [
        "Modified - Using GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m89RoQ84P8n3",
        "outputId": "2987d67b-1780-4a91-e14d-5ec561fd8182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60, Loss: 3.001787543296814\n",
            "Epoch 2/60, Loss: 2.599470615386963\n",
            "Epoch 3/60, Loss: 2.19442880153656\n",
            "Epoch 4/60, Loss: 1.9167596101760864\n",
            "Epoch 5/60, Loss: 1.8824473023414612\n",
            "Epoch 6/60, Loss: 2.0172500014305115\n",
            "Epoch 7/60, Loss: 1.7893186211585999\n",
            "Epoch 8/60, Loss: 1.5327982902526855\n",
            "Epoch 9/60, Loss: 1.4570062756538391\n",
            "Epoch 10/60, Loss: 1.315062701702118\n",
            "Epoch 11/60, Loss: 1.4665381908416748\n",
            "Epoch 12/60, Loss: 1.3962971568107605\n",
            "Epoch 13/60, Loss: 1.6835548877716064\n",
            "Epoch 14/60, Loss: 1.307212233543396\n",
            "Epoch 15/60, Loss: 1.3668845891952515\n",
            "Epoch 16/60, Loss: 1.3635870814323425\n",
            "Epoch 17/60, Loss: 1.431712031364441\n",
            "Epoch 18/60, Loss: 1.2830909490585327\n",
            "Epoch 19/60, Loss: 1.2621662616729736\n",
            "Epoch 20/60, Loss: 1.2991197109222412\n",
            "Epoch 21/60, Loss: 1.165330946445465\n",
            "Epoch 22/60, Loss: 1.2258515357971191\n",
            "Epoch 23/60, Loss: 1.240250825881958\n",
            "Epoch 24/60, Loss: 1.5886485576629639\n",
            "Epoch 25/60, Loss: 1.1873716115951538\n",
            "Epoch 26/60, Loss: 1.2995360493659973\n",
            "Epoch 27/60, Loss: 1.2428520321846008\n",
            "Epoch 28/60, Loss: 1.1579664945602417\n",
            "Epoch 29/60, Loss: 1.26554274559021\n",
            "Epoch 30/60, Loss: 1.3313955664634705\n",
            "Epoch 31/60, Loss: 1.1693745255470276\n",
            "Epoch 32/60, Loss: 1.115979552268982\n",
            "Epoch 33/60, Loss: 1.1746830940246582\n",
            "Epoch 34/60, Loss: 1.3022554516792297\n",
            "Epoch 35/60, Loss: 1.363561987876892\n",
            "Epoch 36/60, Loss: 1.3837338089942932\n",
            "Epoch 37/60, Loss: 1.1886300444602966\n",
            "Epoch 38/60, Loss: 1.3716468811035156\n",
            "Epoch 39/60, Loss: 1.1255533695220947\n",
            "Epoch 40/60, Loss: 1.202873945236206\n",
            "Epoch 41/60, Loss: 1.212728202342987\n",
            "Epoch 42/60, Loss: 1.112711489200592\n",
            "Epoch 43/60, Loss: 1.213855266571045\n",
            "Epoch 44/60, Loss: 1.2411174774169922\n",
            "Epoch 45/60, Loss: 1.1751909255981445\n",
            "Epoch 46/60, Loss: 1.3184837698936462\n",
            "Epoch 47/60, Loss: 1.2458040118217468\n",
            "Epoch 48/60, Loss: 1.105689525604248\n",
            "Epoch 49/60, Loss: 1.242469072341919\n",
            "Epoch 50/60, Loss: 1.0755205154418945\n",
            "Epoch 51/60, Loss: 1.2283419370651245\n",
            "Epoch 52/60, Loss: 1.109120786190033\n",
            "Epoch 53/60, Loss: 1.2598102688789368\n",
            "Epoch 54/60, Loss: 1.0949920415878296\n",
            "Epoch 55/60, Loss: 1.1853405237197876\n",
            "Epoch 56/60, Loss: 1.2669256329536438\n",
            "Epoch 57/60, Loss: 1.1203715205192566\n",
            "Epoch 58/60, Loss: 1.1998462677001953\n",
            "Epoch 59/60, Loss: 1.058383285999298\n",
            "Epoch 60/60, Loss: 1.0922920107841492\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Sample data with placeholders\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 1, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2, 'order_date': '2022-01-15'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrames for the sample data\n",
        "customers_df = pd.DataFrame(sample_data['customers'])\n",
        "orders_df = pd.DataFrame(sample_data['orders'])\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token for GPT-2\n",
        "\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        if 'name' in row and 'email' in row:\n",
        "            input_text = f\"INSERT INTO customers (id, name, email) VALUES ({row['id']}, '{fake.name()}', '{fake.email()}');\"\n",
        "        else:\n",
        "            input_text = f\"INSERT INTO orders (id, customer_id, order_date) VALUES ({row['id']}, {row['customer_id']}, '{row['order_date']}');\"\n",
        "\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
        "        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze()}\n",
        "\n",
        "# Combine datasets and create DataLoader with padding\n",
        "combined_df = pd.concat([customers_df, orders_df], ignore_index=True)\n",
        "dataset = SQLDataset(combined_df, tokenizer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 60\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone()  # GPT-2 generates the next token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader)}')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Generate SQL statements\n",
        "def generate_sql_data_from_trained_model(prompt, num_statements=20):\n",
        "    generated_statements = []\n",
        "    for _ in range(num_statements):\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Post-process to replace 'nan' or invalid data with realistic fake data\n",
        "        generated_sql = generated_sql.replace(\"'nan'\", f\"'{fake.name()}'\")\n",
        "        generated_sql = generated_sql.replace(\"'nan'\", f\"'{fake.email()}'\")\n",
        "        generated_statements.append(generated_sql)\n",
        "\n",
        "    return generated_statements\n",
        "\n",
        "# Generate unique SQL statements\n",
        "def generate_sql_data_not_generalized(num_statements=20):\n",
        "    generated_statements = []\n",
        "    for _ in range(num_statements):\n",
        "        # Generate unique fake data for each statement\n",
        "        id_value = fake.random_int(min=1, max=1000)\n",
        "        name_value = fake.name()\n",
        "        email_value = fake.email()\n",
        "\n",
        "        generated_sql = f\"INSERT INTO customers (id, name, email) VALUES ({id_value}, '{name_value}', '{email_value}');\"\n",
        "        generated_statements.append(generated_sql)\n",
        "\n",
        "    return generated_statements\n",
        "\n",
        "import re\n",
        "\n",
        "def generate_sql_data(prompt, num_statements=20):\n",
        "    generated_statements = []\n",
        "    for _ in range(num_statements):\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        # Create attention mask\n",
        "        inputs['attention_mask'] = (inputs['input_ids'] != tokenizer.pad_token_id).long()\n",
        "\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Define patterns to identify column types\n",
        "        name_pattern = re.compile(r\"\\bname\\b\", re.IGNORECASE)\n",
        "        email_pattern = re.compile(r\"\\bemail\\b\", re.IGNORECASE)\n",
        "        date_pattern = re.compile(r\"\\bdate\\b\", re.IGNORECASE)\n",
        "        id_pattern = re.compile(r\"\\bid\\b\", re.IGNORECASE)\n",
        "\n",
        "        # Post-process to replace 'nan' or invalid data with realistic fake data\n",
        "        def replace_with_fake_data(match):\n",
        "            column_name = match.group(0).lower()\n",
        "            if name_pattern.search(column_name):\n",
        "                return f\"'{fake.name()}'\"\n",
        "            elif email_pattern.search(column_name):\n",
        "                return f\"'{fake.email()}'\"\n",
        "            elif date_pattern.search(column_name):\n",
        "                return f\"'{fake.date()}'\"\n",
        "            elif id_pattern.search(column_name):\n",
        "                return str(fake.random_int(min=1, max=1000))\n",
        "            else:\n",
        "                return f\"'{fake.word()}'\"  # Generic replacement for other columns\n",
        "\n",
        "        # Replace any 'nan' or invalid data by matching with column names\n",
        "        generated_sql = re.sub(r\"'nan'\", replace_with_fake_data, generated_sql)\n",
        "        generated_sql = re.sub(r\"'\\d+'\", replace_with_fake_data, generated_sql)\n",
        "\n",
        "        generated_statements.append(generated_sql)\n",
        "\n",
        "    return generated_statements\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtXEmDfScM7x",
        "outputId": "9134a5ab-4b9d-449c-ba66-f0f596f35ee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INSERT INTO customers (id, name, email) VALUES (1, 'such', 'be');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'another', 'or');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'participant', 'approach');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'magazine', 'number');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'important', 'receive');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'right', 'attention');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'specific', 'nature');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'with', 'start');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'dog', 'report');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'dark', 'six');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'last', 'somebody');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'while', 'can');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'morning', 'store');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'quickly', 'speak');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'in', 'maybe');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'policy', 'world');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'seem', 'put');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'yeah', 'stock');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'way', 'why');\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'official', 'throw');\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "prompt = \"INSERT INTO customers (id, name, email) VALUES (1, 'nan', 'nan');\"\n",
        "sample_sql_statements = generate_sql_data(prompt, num_statements=20)\n",
        "\n",
        "# Print the generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt1zM8Haux5H",
        "outputId": "c2f6a5b1-2920-4a6b-c9f7-6a216b4821be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n",
            "INSERT INTO person (id, fname,lname, email) Values (1, 'David Smith', 'joseph@example.org');\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "prompt = \"INSERT INTO CUSTOMERS (id, fname,lname, email) Values\"\n",
        "sample_sql_statements = generate_sql_data(prompt, num_statements=20)\n",
        "\n",
        "# Print the generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHnxUCEXcOaG"
      },
      "outputs": [],
      "source": [
        "### Another type of prompt\n",
        "input_prompt = \"INSERT INTO customers (id, name, email) VALUES\"\n",
        "sample_sql_statements = generate_sql_data(input_prompt, num_statements=20)\n",
        "\n",
        "# Print generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPLFGvB9HDNL"
      },
      "source": [
        "Updated for more dynamic generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "njstSGVAHGEx",
        "outputId": "74560d7a-d070-407f-e5f4-1e7428089da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60, Loss: 3.3150335550308228\n",
            "Epoch 2/60, Loss: 3.0263644456863403\n",
            "Epoch 3/60, Loss: 2.67734158039093\n",
            "Epoch 4/60, Loss: 2.312158703804016\n",
            "Epoch 5/60, Loss: 2.1938542127609253\n",
            "Epoch 6/60, Loss: 1.8724486827850342\n",
            "Epoch 7/60, Loss: 1.7022778987884521\n",
            "Epoch 8/60, Loss: 1.7141201496124268\n",
            "Epoch 9/60, Loss: 1.5675982236862183\n",
            "Epoch 10/60, Loss: 1.3163073658943176\n",
            "Epoch 11/60, Loss: 1.144919216632843\n",
            "Epoch 12/60, Loss: 1.138761281967163\n",
            "Epoch 13/60, Loss: 0.99598827958107\n",
            "Epoch 14/60, Loss: 0.9684582352638245\n",
            "Epoch 15/60, Loss: 1.0192618072032928\n",
            "Epoch 16/60, Loss: 0.9338520169258118\n",
            "Epoch 17/60, Loss: 1.017622172832489\n",
            "Epoch 18/60, Loss: 0.917255699634552\n",
            "Epoch 19/60, Loss: 0.8865472078323364\n",
            "Epoch 20/60, Loss: 0.9768170118331909\n",
            "Epoch 21/60, Loss: 0.8881572782993317\n",
            "Epoch 22/60, Loss: 0.8067397773265839\n",
            "Epoch 23/60, Loss: 0.8851421773433685\n",
            "Epoch 24/60, Loss: 0.7968523800373077\n",
            "Epoch 25/60, Loss: 0.8314014375209808\n",
            "Epoch 26/60, Loss: 0.7426825761795044\n",
            "Epoch 27/60, Loss: 0.7339727878570557\n",
            "Epoch 28/60, Loss: 0.7151519060134888\n",
            "Epoch 29/60, Loss: 0.7637204527854919\n",
            "Epoch 30/60, Loss: 0.7259450852870941\n",
            "Epoch 31/60, Loss: 0.7366973161697388\n",
            "Epoch 32/60, Loss: 0.7757704854011536\n",
            "Epoch 33/60, Loss: 0.7239257097244263\n",
            "Epoch 34/60, Loss: 0.7978082299232483\n",
            "Epoch 35/60, Loss: 0.8193000853061676\n",
            "Epoch 36/60, Loss: 0.6941552460193634\n",
            "Epoch 37/60, Loss: 0.7581366300582886\n",
            "Epoch 38/60, Loss: 0.7119030952453613\n",
            "Epoch 39/60, Loss: 0.7684503793716431\n",
            "Epoch 40/60, Loss: 0.8423243761062622\n",
            "Epoch 41/60, Loss: 0.8041686713695526\n",
            "Epoch 42/60, Loss: 0.7034293413162231\n",
            "Epoch 43/60, Loss: 0.735560953617096\n",
            "Epoch 44/60, Loss: 0.8935962319374084\n",
            "Epoch 45/60, Loss: 0.8133420944213867\n",
            "Epoch 46/60, Loss: 0.7583024203777313\n",
            "Epoch 47/60, Loss: 0.7486731708049774\n",
            "Epoch 48/60, Loss: 0.7752116024494171\n",
            "Epoch 49/60, Loss: 0.7276479601860046\n",
            "Epoch 50/60, Loss: 0.760773092508316\n",
            "Epoch 51/60, Loss: 0.7601446509361267\n",
            "Epoch 52/60, Loss: 0.7129808962345123\n",
            "Epoch 53/60, Loss: 0.7429589927196503\n",
            "Epoch 54/60, Loss: 0.7931127846240997\n",
            "Epoch 55/60, Loss: 0.7289723753929138\n",
            "Epoch 56/60, Loss: 0.6911588311195374\n",
            "Epoch 57/60, Loss: 0.6664524376392365\n",
            "Epoch 58/60, Loss: 0.7356640994548798\n",
            "Epoch 59/60, Loss: 0.8003352284431458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/60, Loss: 0.7245734333992004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INSERT INTO customers (id, name, email) VALUES (1, 'staff', 'direction');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'agreement', 'field');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'ask', 'before');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'safe', 'work');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'sea', 'full');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'possible', 'since');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'teach', 'southern');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'daughter', 'bill');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'us', 'candidate');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'check', 'whole');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'defense', 'fight');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'which', 'we');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'available', 'wife');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'pressure', 'meeting');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'realize', 'rock');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'southern', 'have');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'job', 'those');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'entire', 'information');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'form', 'role');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n",
            "INSERT INTO customers (id, name, email) VALUES (1, 'there', 'however');\n",
            "\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Sample data with placeholders\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 1, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2, 'order_date': '2022-01-15'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrames for the sample data\n",
        "customers_df = pd.DataFrame(sample_data['customers'])\n",
        "orders_df = pd.DataFrame(sample_data['orders'])\n",
        "\n",
        "# Assign a 'name' attribute to DataFrames to represent table names (Updated)\n",
        "customers_df.name = 'customers'\n",
        "orders_df.name = 'orders'\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token for GPT-2\n",
        "\n",
        "# Updated SQLDataset class to make SQL generation dynamic\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        table_name = self.df.name  # Dynamic table name (Updated)\n",
        "        columns = row.index.tolist()  # Get column names from the DataFrame (Updated)\n",
        "        values = []\n",
        "\n",
        "        # Generate fake data for each column dynamically (Updated)\n",
        "        for column in columns:\n",
        "            if 'name' in column.lower():\n",
        "                values.append(f\"'{fake.name()}'\")\n",
        "            elif 'email' in column.lower():\n",
        "                values.append(f\"'{fake.email()}'\")\n",
        "            elif 'date' in column.lower():\n",
        "                values.append(f\"'{fake.date()}'\")\n",
        "            elif 'id' in column.lower():\n",
        "                values.append(str(fake.random_int(min=1, max=1000)))\n",
        "            else:\n",
        "                values.append(f\"'{fake.word()}'\")  # Generic replacement for other columns\n",
        "\n",
        "        # Create the SQL insert statement dynamically (Updated)\n",
        "        input_text = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(values)});\"\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
        "        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze()}\n",
        "\n",
        "# Combine datasets and create DataLoader with padding\n",
        "combined_df = pd.concat([customers_df, orders_df], ignore_index=True)\n",
        "dataset = SQLDataset(combined_df, tokenizer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 60\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone()  # GPT-2 generates the next token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader)}')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Generate SQL statements based on the prompt\n",
        "def generate_sql_data(prompt, num_statements=20):\n",
        "    generated_statements = []\n",
        "    for _ in range(num_statements):\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Define patterns to identify column types dynamically (Updated)\n",
        "        name_pattern = re.compile(r\"\\bname\\b\", re.IGNORECASE)\n",
        "        email_pattern = re.compile(r\"\\bemail\\b\", re.IGNORECASE)\n",
        "        date_pattern = re.compile(r\"\\bdate\\b\", re.IGNORECASE)\n",
        "        id_pattern = re.compile(r\"\\bid\\b\", re.IGNORECASE)\n",
        "\n",
        "        # Post-process to replace 'nan' or invalid data with realistic fake data dynamically (Updated)\n",
        "        def replace_with_fake_data(match):\n",
        "            column_name = match.group(0).lower()\n",
        "            if name_pattern.search(column_name):\n",
        "                return f\"'{fake.name()}'\"\n",
        "            elif email_pattern.search(column_name):\n",
        "                return f\"'{fake.email()}'\"\n",
        "            elif date_pattern.search(column_name):\n",
        "                return f\"'{fake.date()}'\"\n",
        "            elif id_pattern.search(column_name):\n",
        "                return str(fake.random_int(min=1, max=1000))\n",
        "            else:\n",
        "                return f\"'{fake.word()}'\"  # Generic replacement for other columns\n",
        "\n",
        "        # Replace any 'nan' or invalid data by matching with column names (Updated)\n",
        "        generated_sql = re.sub(r\"'nan'\", replace_with_fake_data, generated_sql)\n",
        "        generated_sql = re.sub(r\"'\\d+'\", replace_with_fake_data, generated_sql)\n",
        "\n",
        "        generated_statements.append(generated_sql)\n",
        "\n",
        "    return generated_statements\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"INSERT INTO customers (id, name, email) VALUES (1, 'nan', 'nan');\"\n",
        "sample_sql_statements = generate_sql_data(prompt, num_statements=20)\n",
        "\n",
        "# Print the generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLtaQfwzHDBp"
      },
      "source": [
        "# Above is failed implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddI4_V2q-zr8",
        "outputId": "753161b1-7a6e-4dd3-85d8-de591c857ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJwfHeTJBhws",
        "outputId": "4ff7e76b-88be-4c6a-9d65-42858dbcde07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (27.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyq6F9vRHCvm",
        "outputId": "4853821b-1793-41ed-fbd3-b9a81d45baf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.7288522720336914\n",
            "Epoch 2/100, Loss: 2.306886315345764\n",
            "Epoch 3/100, Loss: 1.9932240545749664\n",
            "Epoch 4/100, Loss: 1.804379642009735\n",
            "Epoch 5/100, Loss: 1.6513957977294922\n",
            "Epoch 6/100, Loss: 1.375845581293106\n",
            "Epoch 7/100, Loss: 1.3221988081932068\n",
            "Epoch 8/100, Loss: 1.2304116785526276\n",
            "Epoch 9/100, Loss: 1.1590915024280548\n",
            "Epoch 10/100, Loss: 1.1357654333114624\n",
            "Epoch 11/100, Loss: 1.0799949318170547\n",
            "Epoch 12/100, Loss: 1.03450246155262\n",
            "Epoch 13/100, Loss: 1.0222009420394897\n",
            "Epoch 14/100, Loss: 0.9552304893732071\n",
            "Epoch 15/100, Loss: 0.955097883939743\n",
            "Epoch 16/100, Loss: 0.9440721273422241\n",
            "Epoch 17/100, Loss: 0.9252956509590149\n",
            "Epoch 18/100, Loss: 0.8740400671958923\n",
            "Epoch 19/100, Loss: 0.9478059411048889\n",
            "Epoch 20/100, Loss: 0.9007769972085953\n",
            "Epoch 21/100, Loss: 0.853043720126152\n",
            "Epoch 22/100, Loss: 0.9251676052808762\n",
            "Epoch 23/100, Loss: 0.8781739920377731\n",
            "Epoch 24/100, Loss: 0.8630498498678207\n",
            "Epoch 25/100, Loss: 0.8842781484127045\n",
            "Epoch 26/100, Loss: 0.8767949491739273\n",
            "Epoch 27/100, Loss: 0.79110948741436\n",
            "Epoch 28/100, Loss: 0.8086143285036087\n",
            "Epoch 29/100, Loss: 0.8119179755449295\n",
            "Epoch 30/100, Loss: 0.8532546609640121\n",
            "Epoch 31/100, Loss: 0.8044251352548599\n",
            "Epoch 32/100, Loss: 0.8444220125675201\n",
            "Epoch 33/100, Loss: 0.8278308361768723\n",
            "Epoch 34/100, Loss: 0.8363721966743469\n",
            "Epoch 35/100, Loss: 0.8294620215892792\n",
            "Epoch 36/100, Loss: 0.8668622523546219\n",
            "Epoch 37/100, Loss: 0.8043432831764221\n",
            "Epoch 38/100, Loss: 0.8092416524887085\n",
            "Epoch 39/100, Loss: 0.8273069709539413\n",
            "Epoch 40/100, Loss: 0.7547773271799088\n",
            "Epoch 41/100, Loss: 0.8188505470752716\n",
            "Epoch 42/100, Loss: 0.7830891758203506\n",
            "Epoch 43/100, Loss: 0.8690644055604935\n",
            "Epoch 44/100, Loss: 0.7819991856813431\n",
            "Epoch 45/100, Loss: 0.7606198191642761\n",
            "Epoch 46/100, Loss: 0.8365688323974609\n",
            "Epoch 47/100, Loss: 0.8041302412748337\n",
            "Epoch 48/100, Loss: 0.8025941699743271\n",
            "Epoch 49/100, Loss: 0.760495200753212\n",
            "Epoch 50/100, Loss: 0.7966826558113098\n",
            "Epoch 51/100, Loss: 0.7647955119609833\n",
            "Epoch 52/100, Loss: 0.7751610428094864\n",
            "Epoch 53/100, Loss: 0.7954874634742737\n",
            "Epoch 54/100, Loss: 0.8321320116519928\n",
            "Epoch 55/100, Loss: 0.7778968513011932\n",
            "Epoch 56/100, Loss: 0.7522609680891037\n",
            "Epoch 57/100, Loss: 0.7775566726922989\n",
            "Epoch 58/100, Loss: 0.772413358092308\n",
            "Epoch 59/100, Loss: 0.7536723017692566\n",
            "Epoch 60/100, Loss: 0.7528557032346725\n",
            "Epoch 61/100, Loss: 0.7874628454446793\n",
            "Epoch 62/100, Loss: 0.7732158154249191\n",
            "Epoch 63/100, Loss: 0.7781610488891602\n",
            "Epoch 64/100, Loss: 0.8278033584356308\n",
            "Epoch 65/100, Loss: 0.773139476776123\n",
            "Epoch 66/100, Loss: 0.7430768311023712\n",
            "Epoch 67/100, Loss: 0.7794809341430664\n",
            "Epoch 68/100, Loss: 0.7587495595216751\n",
            "Epoch 69/100, Loss: 0.7738492041826248\n",
            "Epoch 70/100, Loss: 0.7816181778907776\n",
            "Epoch 71/100, Loss: 0.7095296382904053\n",
            "Epoch 72/100, Loss: 0.7933925837278366\n",
            "Epoch 73/100, Loss: 0.7662100046873093\n",
            "Epoch 74/100, Loss: 0.7801171243190765\n",
            "Epoch 75/100, Loss: 0.780087411403656\n",
            "Epoch 76/100, Loss: 0.7308982163667679\n",
            "Epoch 77/100, Loss: 0.7719724923372269\n",
            "Epoch 78/100, Loss: 0.7228395044803619\n",
            "Epoch 79/100, Loss: 0.7221448123455048\n",
            "Epoch 80/100, Loss: 0.7586718201637268\n",
            "Epoch 81/100, Loss: 0.736341804265976\n",
            "Epoch 82/100, Loss: 0.7302930504083633\n",
            "Epoch 83/100, Loss: 0.780299037694931\n",
            "Epoch 84/100, Loss: 0.7284553647041321\n",
            "Epoch 85/100, Loss: 0.7854862809181213\n",
            "Epoch 86/100, Loss: 0.7655033618211746\n",
            "Epoch 87/100, Loss: 0.7589324861764908\n",
            "Epoch 88/100, Loss: 0.763441413640976\n",
            "Epoch 89/100, Loss: 0.7575214356184006\n",
            "Epoch 90/100, Loss: 0.7415846586227417\n",
            "Epoch 91/100, Loss: 0.7286546975374222\n",
            "Epoch 92/100, Loss: 0.6967413425445557\n",
            "Epoch 93/100, Loss: 0.7441676110029221\n",
            "Epoch 94/100, Loss: 0.7608972787857056\n",
            "Epoch 95/100, Loss: 0.6947208940982819\n",
            "Epoch 96/100, Loss: 0.7320152074098587\n",
            "Epoch 97/100, Loss: 0.7304919064044952\n",
            "Epoch 98/100, Loss: 0.7207786738872528\n",
            "Epoch 99/100, Loss: 0.7898736745119095\n",
            "Epoch 100/100, Loss: 0.7279932200908661\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/NLP/SavedModels/SqlGenerator/gpt2_sql_generator.pth\n",
            "Tokenizer saved to /content/drive/MyDrive/Colab Notebooks/NLP/SavedModels/SqlGenerator/gpt2_tokenizer\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from faker import Faker\n",
        "import re  # Import the `re` module\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "Faker.seed(4321)\n",
        "\n",
        "# Sample data with placeholders\n",
        "sample_data = {\n",
        "    'customers': [\n",
        "        {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'},\n",
        "        {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}\n",
        "    ],\n",
        "    'orders': [\n",
        "        {'id': 1, 'customer_id': 10001, 'order_date': '2022-01-01'},\n",
        "        {'id': 2, 'customer_id': 2001, 'order_date': '2023-01-15'},\n",
        "        {'id': 34, 'customer_id': 101, 'order_date': '2013-01-11'}\n",
        "    ],\n",
        "    'product': [\n",
        "        {'skuid': '1231', 'desc': \"electronics\", 'inventory_count': 100, 'refill_date':'2024-01-01'},\n",
        "        {'skuid': '2a23', 'desc': \"furniture\", 'inventory_count': 568 , 'refill_date':'2024-01-01'},\n",
        "        {'skuid': '3awsd4', 'desc': \"miscellaneous\", 'inventory_count': 34562,'refill_date':'2024-01-01'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrames for the sample data\n",
        "customers_df = pd.DataFrame(sample_data['customers'])\n",
        "orders_df = pd.DataFrame(sample_data['orders'])\n",
        "product_df = pd.DataFrame(sample_data['product'])\n",
        "\n",
        "# Assign a 'name' attribute to DataFrames to represent table names\n",
        "customers_df.name = 'customers'\n",
        "orders_df.name = 'orders'\n",
        "product_df.name = 'product'\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2-medium'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token for GPT-2\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# SQLDataset class for dynamic SQL generation\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        table_name = self.df.name  # Dynamic table name\n",
        "        columns = row.index.tolist()  # Get column names from the DataFrame\n",
        "        values = []\n",
        "\n",
        "        # Generate fake data for each column dynamically\n",
        "        for column in columns:\n",
        "            if 'name' in column.lower():\n",
        "                values.append(f\"'{fake.name()}'\")\n",
        "            elif 'skuid' in column.lower():\n",
        "                values.append(f\"'{fake.bothify(letters='aedhfcwsqikdr')}'\")\n",
        "            elif 'email' in column.lower():\n",
        "                values.append(f\"'{fake.email()}'\")\n",
        "            elif 'date' in column.lower():\n",
        "                values.append(f\"'{fake.date()}'\")\n",
        "            elif 'desc' in column.lower():\n",
        "                values.append(f\"'{fake.paragraph(nb_sentences=1)}'\")\n",
        "            elif 'id' in column.lower():\n",
        "                values.append(str(fake.random_int(min=1, max=1000)))\n",
        "            elif 'inventory_count' in column.lower():\n",
        "                values.append(str(fake.random_int(min=1, max=1000)))\n",
        "            else:\n",
        "                values.append(f\"'{fake.word()}'\")  # Generic replacement for other columns\n",
        "\n",
        "        # Create the SQL insert statement dynamically\n",
        "        input_text = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(values)});\"\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
        "        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze()}\n",
        "\n",
        "# Combine datasets and create DataLoader with padding\n",
        "combined_df = pd.concat([customers_df, orders_df, product_df], ignore_index=True)\n",
        "dataset = SQLDataset(combined_df, tokenizer)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Fine tuning - Training the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone()  # GPT-2 generates the next token\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader)}')\n",
        "\n",
        "\n",
        "\n",
        "# Save the model's state dictionary and tokenizer\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/NLP/SavedModels/SqlGenerator/gpt2_sql_generator.pth'\n",
        "tokenizer_save_path = '/content/drive/MyDrive/Colab Notebooks/NLP/SavedModels/SqlGenerator/gpt2_tokenizer'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "print(f\"Tokenizer saved to {tokenizer_save_path}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Generate SQL statements based on the prompt\n",
        "def generate_sql_data(prompt, num_statements=20):\n",
        "    generated_statements = []\n",
        "    for _ in range(num_statements):\n",
        "        # Tokenize the input prompt\n",
        "        inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "\n",
        "        # Create the attention mask\n",
        "        inputs['attention_mask'] = (inputs['input_ids'] != tokenizer.pad_token_id).long()\n",
        "\n",
        "        # ignore attention for padding characters\n",
        "\n",
        "        # Generate the output\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],  # Pass attention_mask here\n",
        "            max_length=1000,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id  # Ensure the model knows about pad_token_id\n",
        "        )\n",
        "\n",
        "        # outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
        "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Decode and print the output for debugging\n",
        "        print(\"Raw Generated Output:\", generated_sql)  # Debugging line\n",
        "\n",
        "        # Define patterns to identify column types dynamically\n",
        "        name_pattern = re.compile(r\"\\bname\\b\", re.IGNORECASE)\n",
        "        email_pattern = re.compile(r\"\\bemail\\b\", re.IGNORECASE)\n",
        "        date_pattern = re.compile(r\"\\bdate\\b\", re.IGNORECASE)\n",
        "        id_pattern = re.compile(r\"\\bid\\b\", re.IGNORECASE)\n",
        "        desc_pattern = re.compile(r\"\\bdesc\\b\", re.IGNORECASE)\n",
        "        sku_pattern = re.compile(r\"\\bsku\\b\", re.IGNORECASE)\n",
        "        ssn_pattern = re.compile(r\"\\bssn\\b\", re.IGNORECASE)\n",
        "\n",
        "        # Post-process to replace 'nan' or invalid data with realistic fake data\n",
        "        def replace_with_fake_data(match):\n",
        "            column_name = match.group(0).lower()\n",
        "            if name_pattern.search(column_name):\n",
        "                return f\"'{fake.name()}'\"\n",
        "            elif email_pattern.search(column_name):\n",
        "                return f\"'{fake.email()}'\"\n",
        "            elif date_pattern.search(column_name):\n",
        "                return f\"'{fake.date()}'\"\n",
        "            elif id_pattern.search(column_name):\n",
        "                return str(fake.random_int(min=1, max=1000))\n",
        "            elif desc_pattern.search(column_name):\n",
        "                return f\"'{fake.paragraph(nb_sentences=1)}'\"\n",
        "            elif sku_pattern.search(column_name):\n",
        "                return str(fake.bothify(letters='aedhfcwsqikdr'))\n",
        "            elif ssn_pattern.search(column_name):\n",
        "                return str(fake.ssn())\n",
        "            else:\n",
        "                return f\"'{fake.word()}'\"  # Generic replacement for other columns\n",
        "\n",
        "        # Replace any 'nan' or invalid data by matching with column names\n",
        "        generated_sql = re.sub(r\"'nan'\", replace_with_fake_data, generated_sql)\n",
        "\n",
        "        generated_statements.append(generated_sql)\n",
        "\n",
        "    return generated_statements\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "prompt = \"INSERT INTO person (id, name, email, description)\"\n",
        "sample_sql_statements = generate_sql_data(prompt, num_statements=20)\n",
        "\n",
        "# Print the generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r13GE3OEevFY",
        "outputId": "21e5cea3-e08d-4e4b-cea3-5c9a1b16678f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "Raw Generated Output: INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n",
            "INSERT INTO person (id, name, email, description) VALUES (9, 'Michael Brown','michael@example.org', 5, '2008-04-24', '20 hd', 'Military officer.', 5, '1970-08-24');\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"create table Employee \"\n",
        "sample_sql_statements = generate_sql_data(prompt, num_statements=1)\n",
        "\n",
        "# Print the generated SQL statements\n",
        "for statement in sample_sql_statements:\n",
        "    print(statement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIvUZlersZqO",
        "outputId": "db2cad84-85bb-40a0-ff9f-8fdd82105d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Generated Output: create table Employee ( name, email, customer_id, order_date, skuid, desc, inventory_count, refill_date) VALUES (821, 'Michael Brown','michael@example.org', 597, '1986-03-24', '20 hd', 'Carry out a person.', 597, '2023-06-24');\n",
            "create table Employee ( name, email, customer_id, order_date, skuid, desc, inventory_count, refill_date) VALUES (821, 'Michael Brown','michael@example.org', 597, '1986-03-24', '20 hd', 'Carry out a person.', 597, '2023-06-24');\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-DGvvib-_o"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8E8DmxBP5pt"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNQ63m08hOmixVqVf7kzE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}