{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO37YyPl+b7uFoJLwPL5CSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhareesha/MLAI/blob/main/NN/NN_forward_backward_prop_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "heSYYMMY7AjT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)"
      ],
      "metadata": {
        "id": "vmlT9GALenCx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_direct(Z):\n",
        "  softmax = np.exp(Z) / np.sum(np.exp(Z))\n",
        "  return softmax"
      ],
      "metadata": {
        "id": "AuzYsBb2UIXk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax with numerical stability\n",
        "\n",
        "When Z values are very large or very small, there is a change of $exp$ to be Infinity or zero.In order to avoid it we always subtract max value from each z value, so that the value shifts left."
      ],
      "metadata": {
        "id": "ENRWhFP3XMj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "  z_exp = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
        "  softmax = z_exp / np.sum(z_exp, axis = 0, keepdims = True)\n",
        "  return softmax"
      ],
      "metadata": {
        "id": "Ezc1uCcSWqcP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyOWk3zFplHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Number of paramters do not depend up on number of training examples\n",
        "- They do depend up on number of feature.\n",
        "- For one training example, W is a ROW vector $[w_1, w_2, w_3 ... w_n]$.\n",
        "- For one trainig example X is a COLUMN vector $[x_1, x_2,x_3...x_m]^T$.\n",
        "- $ Y = WX + B $ linear combination of inputs"
      ],
      "metadata": {
        "id": "zOSQWSRyh0rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "A simple implementation of a 2-layer neural network using NumPy in Python, which includes an input layer with 5 inputs(5 features), two hidden layers each with 10 neurons using the ReLU activation function, and an output layer with 4 neurons.\n",
        "\n",
        "#### Notation\n",
        "$L^{[layer number]}$\n",
        "\n",
        "For $m$ traning example\n",
        "\n",
        "Input Layer - $ X_{5xm}$  which is same as $A_{5xm}^{[0]}$\n",
        "\n",
        "$\\\\\\\\$\n",
        "\n",
        "Hidden Layer $L^{[1]}$ has  $A_{10xm}^{[1]} = f(W_{10 x 5}^{[1]}A_{5 x m}^{[0]} + B_{10 x 1}^{[1]})$\n",
        "\n",
        "$\\\\\\\\$\n",
        "\n",
        "Hidden Layer $L^{[2]}$ has $A_{10xm}^{[2]} = f(W_{10 x 10}^{[2]}A_{10 x m}^{[1]} + B_{10 x 1}^{[2]})$\n",
        "\n",
        "$\\\\\\\\$\n",
        "\n",
        "Here Z can be generalized to $Z = WA + B $ and $A = f(Z)$\n",
        "\n",
        "\n",
        "$\\\\\\\\$\n",
        "\n",
        "Output layer is a Softmax layer\n",
        "\n",
        "$A_{4xm}^{[3]} = softmax(W_{4x10}^{[3]}A_{10xm}^{[2]}+B_{4x1}^{[3]})$\n",
        "\n",
        "\n",
        "### Matrices at a given Layer\n",
        "\n",
        "$Z^{[2]} = W^{[2]} \\times A^{[1]} + B^{[2]}$\n",
        "\n",
        "$A^{[2]} = f(Z^{[2]})$\n",
        "\n",
        "Here\n",
        "\n",
        "$A^{[1]}$ : activations(inputs) from preceding layer, which here is Layer-1\n",
        "\n",
        "$Z^{[2]}$ : The linear output (pre-activation) of the second layer.\n",
        "\n",
        "$W^{[2]}$ : The weight matrix for the second layer.\n",
        "\n",
        "$B^{[2]}$ : The bias vector for the second layer.\n",
        "\n",
        "### Dimensions\n",
        "If $n_1$ is number of neurons in $1_{st}$ layer and $n_2$ is number of neurons in $2_{nd}$ layer and $m$ is number of training examples, then dimensions would be\n",
        "\n",
        "- $W^{[2]}$ will have dimensions $n_2×n_1$ , because each of $n_2$ neurons of second layer connects to all neurons of first layer $n_1$\n",
        "- $B^{[2]}$ will have dimension $n_2 × 1$,as each neuron in the second layer has a single bias term\n",
        "- $A^{[1]}$ will have dimension $n_1 × m$, where each column represents the activations from the first layer for one training example.\n",
        "- $A^{[2]}$ will have dimension $n_2 × m$, where each column represents the activations from the second layer for one training example.\n",
        "\n",
        "\n",
        "$\n",
        "W^{[2]} = \\begin{bmatrix}\n",
        "w_{11}^{[2]} & w_{12}^{[2]} & \\cdots & w_{1n_1}^{[2]} \\\\\n",
        "w_{21}^{[2]} & w_{22}^{[2]} & \\cdots & w_{2n_1}^{[2]} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{n_2 1}^{[2]} & w_{n_2 2}^{[2]} & \\cdots & w_{n_2 n_1}^{[2]}\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "$w_{ij}^{[2]}$ : Weight connecting the $j_{th}$ neuron of the first layer to the $i_{th}$ neuron of the second layer.\n",
        "\n",
        "\n",
        "$\n",
        "A^{[1]} = \\begin{bmatrix}\n",
        "a_{11}^{[1]} & a_{12}^{[1]} & \\cdots & a_{1m}^{[1]} \\\\\n",
        "a_{21}^{[1]} & a_{22}^{[1]} & \\cdots & a_{2m}^{[1]} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n_1 1}^{[1]} & a_{n_1 2}^{[1]} & \\cdots & a_{n_1 m}^{[1]}\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Each column corresponds to the activations from the first layer for a specific training example.\n",
        "\n",
        "$\n",
        "Z^{[2]} = \\begin{bmatrix}\n",
        "z_{11}^{[2]} & z_{12}^{[2]} & \\cdots & z_{1m}^{[2]} \\\\\n",
        "z_{21}^{[2]} & z_{22}^{[2]} & \\cdots & z_{2m}^{[2]} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "z_{n_2 1}^{[2]} & z_{n_2 2}^{[2]} & \\cdots & z_{n_2 m}^{[2]}\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Each column corresponds to the linear output for a specific training example\n",
        "\n",
        "\n",
        "$\n",
        "b^{[2]} = \\begin{bmatrix}\n",
        "b_{1}^{[2]} \\\\\n",
        "b_{2}^{[2]} \\\\\n",
        "\\vdots \\\\\n",
        "b_{n_2}^{[2]}\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Each element corresponds to the bias term for a neuron in the second layer."
      ],
      "metadata": {
        "id": "MfLiSPxLiQM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing parameters for 2 hidden layers and one output layer"
      ],
      "metadata": {
        "id": "T64EYLDGiR1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# W1,b1,W2,b2,W3,b3\n",
        "\n",
        "def init_parameters():\n",
        "  np.random.seed(1)\n",
        "  # Layer - 1\n",
        "  W1 = np.random.randn(10, 5) * 0.01\n",
        "  b1 = np.random.randn(10, 1) * 0.01\n",
        "\n",
        "  # Layer - 2\n",
        "  W2 = np.random.randn(10, 10) * 0.01\n",
        "  b2 = np.random.randn(10, 1) * 0.01\n",
        "\n",
        "  # Layer - 3 (output layer)\n",
        "  W3 = np.random.randn(4, 10) * 0.01\n",
        "  b3 = np.random.randn(4, 1) * 0.01\n",
        "\n",
        "  parameters = (W1, b1, W2, b2, W3, b3)\n",
        "  return parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "obZhaHMehmpa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation\n",
        "\n",
        "At high level\n",
        "\n",
        "$ Z = WA + B$\n",
        "\n",
        "$ A = f(Z) $\n",
        "\n"
      ],
      "metadata": {
        "id": "cT0iC7cUBjRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X, parameters):\n",
        "  W1, b1, W2, b2, W3, b3 = parameters\n",
        "\n",
        "  Z1 = W1 @ X + b1 # Here Z1 will have dimension 10 by m , where m - number of training examples\n",
        "  A1 = relu(Z1) # relu is activation over linear values, A1 will be of size 10 by m\n",
        "\n",
        "  Z2 = W2 @ A1 + b2\n",
        "  A2 = relu(Z2)\n",
        "\n",
        "  Z3 = W3 @ A2 + b3\n",
        "  A3 = softmax(Z3)\n",
        "\n",
        "  pass_activations = (Z1, A1, Z2, A2, Z3, A3)\n",
        "  return A3, pass_activations\n"
      ],
      "metadata": {
        "id": "P0RMdweiBh8-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "                (W1, b1) --> Z1--> A1\n",
        "                                    \\\n",
        "                                     \\\n",
        "                                      \\                   \n",
        "                          (W2, b2) --> Z2 --> A2  \n",
        "                                               \\\n",
        "                                                \\\n",
        "                                   (W3, b3) -->  Z3 --> A3 --> L\n",
        "\n",
        "\n",
        "## Dependency path\n",
        "\n",
        "$ L = Y - A^{[3]}$\n",
        "\n",
        "$Z^{[3]} = W^{[3]} \\cdot A^{[2]} + b^{[3]}$\n",
        "\n",
        "$A^{[3]} = Softmax(Z^{[3]}) $\n",
        "\n",
        "\n",
        "$Z^{[2]} = W^{[2]} \\cdot A^{[1]} + b^{[2]}$\n",
        "\n",
        "$A^{[2]} = ReLU(Z^{[2]}) $\n",
        "\n",
        "\n",
        "$Z^{[1]} = W^{[1]} \\cdot A^{[0]} + b^{[1]}$\n",
        "\n",
        "$A^{[1]} = ReLU(Z^{[1]}) $\n",
        "                              \n",
        "                              \n",
        "                              \n",
        "                                "
      ],
      "metadata": {
        "id": "AU76gdmLBL5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(Z):\n",
        "  return Z > 0"
      ],
      "metadata": {
        "id": "jajBEgQlEuEP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights and Biases adjustment\n",
        "\n",
        "$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "$b_{\\text{new}} = b_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "$\\eta $ is learning rate.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w}$ partial derivative of Loss function wrt to weight($w$), meaning except weight, everything else is treated as constant.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}$ partial derivative of Loss function wrt to bias($b$), meaning except bias, everything else is treated as constant.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9KMFcxyhqPaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(X, Y, parameters, forwardpass_activations):\n",
        "  m = X.shape[1]\n",
        "  W1, b1, W2, b2, W3, b3 = parameters\n",
        "  Z1, A1, Z2, A2, Z3, A3 = forwardpass_activations\n",
        "\n",
        "  # Need to nudge parameters in such a way that L function reduces\n",
        "\n",
        "  # layer - 3\n",
        "  dL_dZ3 = A3 - Y\n",
        "\n",
        "  dZ3_dW3 = A2.T\n",
        "  dL_dW3 = (1/m) * np.dot(dL_dZ3, dZ3_dW3)\n",
        "\n",
        "  dZ3_db3 = 1\n",
        "  # dL_db3 = (1/m) * np.sum(np.dot(dL_dZ3 ,dZ3_db3), axis=1, keepdims=True)\n",
        "  dL_db3 = (1/m) * np.sum(dL_dZ3 , axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "  # LAYER - 2\n",
        "  dZ3_dA2 = W3.T\n",
        "  dL_dA2 = np.dot(dZ3_dA2, dL_dZ3) # Switched positions\n",
        "\n",
        "  dA2_dZ2 = relu_derivative(Z2) # Here A2 is f(z2)\n",
        "  dL_dZ2 = dL_dA2 * dA2_dZ2\n",
        "\n",
        "  dZ2_dW2 = A1.T\n",
        "  dL_dW2 = (1/m) * np.dot(dL_dZ2, dZ2_dW2)\n",
        "\n",
        "  dZ2_db2 = 1\n",
        "  # dL_db2 = (1/m) * np.sum(np.dot(dL_dZ2, dZ2_db2), axis=1, keepdims=True)\n",
        "  dL_db2 = (1/m) * np.sum(dL_dZ2, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "  # LAYER - 1\n",
        "  dZ2_dA1 = W2.T\n",
        "  dL_dA1 = np.dot(dZ2_dA1, dL_dZ2) # SWITCHED POSTITIONS\n",
        "\n",
        "  dA1_dZ1 = relu_derivative(Z1) # Here A2 is f(z2)\n",
        "  dL_dZ1 = dL_dA1 * dA1_dZ1\n",
        "\n",
        "  dZ1_dW1 = X.T\n",
        "  dL_dW1 = (1/m) * np.dot(dL_dZ1 , dZ1_dW1)\n",
        "\n",
        "  dZ1_db1 = 1\n",
        "  # dL_db1 = (1/m) * np.sum(np.dot(dL_dZ1 * dZ1_db1), axis=1, keepdims=True)\n",
        "  dL_db1 = (1/m) * np.sum(dL_dZ1, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "  pass_gradients = (dL_dW1, dL_db1, dL_dW2, dL_db2, dL_dW3, dL_db3)\n",
        "  return pass_gradients\n"
      ],
      "metadata": {
        "id": "G27bIQMKAXbr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, pass_gradients, learning_rate):\n",
        "  W1, b1, W2, b2, W3, b3 = parameters\n",
        "  dL_dW1, dL_db1, dL_dW2, dL_db2, dL_dW3, dL_db3 = pass_gradients\n",
        "\n",
        "  W1 = W1 - learning_rate * dL_dW1\n",
        "  b1 = b1 - learning_rate * dL_db1\n",
        "\n",
        "  W2 = W2 - learning_rate * dL_dW2\n",
        "  b2 = b2 - learning_rate * dL_db2\n",
        "\n",
        "  W3 = W3 - learning_rate * dL_dW3\n",
        "  b3 = b3 - learning_rate * dL_db3\n",
        "\n",
        "  updated_params = (W1, b1, W2, b2, W3, b3)\n",
        "  return updated_params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VqUcRgWoRnfH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution"
      ],
      "metadata": {
        "id": "VdCqunVWSXjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "X = np.random.rand(5, 10) # 5 features and 10 training examples\n",
        "\n",
        "Y = np.eye(4)[np.random.choice(4, 10)].T  # One-hot encoded labels for 4 classes\n",
        "\n",
        "parameters = init_parameters()\n",
        "\n",
        "print(\"Intial parameters - \", parameters)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "A3, forwardpass_predictions = forward_prop(X, parameters)\n",
        "\n",
        "gradients = backward_prop(X, Y, parameters, forwardpass_predictions)\n",
        "\n",
        "updated_parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "\n",
        "print(\"Output after one forward pass:\", A3)\n",
        "\n",
        "print(\"Updated parameters after 1st pass - \", updated_parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PreK0617SXTM",
        "outputId": "6dc86d97-fc1f-48f6-cee0-b344bf1246cf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intial parameters -  (array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969,  0.00865408],\n",
            "       [-0.02301539,  0.01744812, -0.00761207,  0.00319039, -0.0024937 ],\n",
            "       [ 0.01462108, -0.02060141, -0.00322417, -0.00384054,  0.01133769],\n",
            "       [-0.01099891, -0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
            "       [-0.01100619,  0.01144724,  0.00901591,  0.00502494,  0.00900856],\n",
            "       [-0.00683728, -0.0012289 , -0.00935769, -0.00267888,  0.00530355],\n",
            "       [-0.00691661, -0.00396754, -0.00687173, -0.00845206, -0.00671246],\n",
            "       [-0.00012665, -0.0111731 ,  0.00234416,  0.01659802,  0.00742044],\n",
            "       [-0.00191836, -0.00887629, -0.00747158,  0.01692455,  0.00050808],\n",
            "       [-0.00636996,  0.00190915,  0.02100255,  0.00120159,  0.00617203]]), array([[ 0.0030017 ],\n",
            "       [-0.0035225 ],\n",
            "       [-0.01142518],\n",
            "       [-0.00349343],\n",
            "       [-0.00208894],\n",
            "       [ 0.00586623],\n",
            "       [ 0.00838983],\n",
            "       [ 0.00931102],\n",
            "       [ 0.00285587],\n",
            "       [ 0.00885141]]), array([[-0.00754398,  0.01252868,  0.0051293 , -0.00298093,  0.00488518,\n",
            "        -0.00075572,  0.01131629,  0.01519817,  0.02185575, -0.01396496],\n",
            "       [-0.01444114, -0.00504466,  0.00160037,  0.00876169,  0.00315635,\n",
            "        -0.02022201, -0.00306204,  0.00827975,  0.00230095,  0.00762011],\n",
            "       [-0.00222328, -0.00200758,  0.00186561,  0.00410052,  0.001983  ,\n",
            "         0.00119009, -0.00670662,  0.00377564,  0.00121821,  0.01129484],\n",
            "       [ 0.01198918,  0.00185156, -0.00375285, -0.0063873 ,  0.00423494,\n",
            "         0.0007734 , -0.00343854,  0.00043597, -0.00620001,  0.00698032],\n",
            "       [-0.00447129,  0.01224508,  0.00403492,  0.00593579, -0.01094912,\n",
            "         0.00169382,  0.00740556, -0.00953701, -0.00266219,  0.00032615],\n",
            "       [-0.01373117,  0.00315159,  0.00846161, -0.00859516,  0.00350546,\n",
            "        -0.01312283, -0.00038696, -0.01615772,  0.01121418,  0.00408901],\n",
            "       [-0.00024617, -0.00775162,  0.01273756,  0.01967102, -0.01857982,\n",
            "         0.01236164,  0.01627651,  0.00338012, -0.01199268,  0.00863345],\n",
            "       [-0.0018092 , -0.00603921, -0.01230058,  0.00550537,  0.00792807,\n",
            "        -0.00623531,  0.00520576, -0.01144341,  0.00801861,  0.00046567],\n",
            "       [-0.0018657 , -0.00101746,  0.00868886,  0.00750412,  0.00529465,\n",
            "         0.00137701,  0.00077821,  0.0061838 ,  0.00232495,  0.00682551],\n",
            "       [-0.00310117, -0.02434838,  0.01038825,  0.0218698 ,  0.00441364,\n",
            "        -0.00100155, -0.00136445, -0.00119054,  0.00017409, -0.01122019]]), array([[-0.00517094],\n",
            "       [-0.00997027],\n",
            "       [ 0.00248799],\n",
            "       [-0.00296641],\n",
            "       [ 0.00495211],\n",
            "       [-0.00174703],\n",
            "       [ 0.00986335],\n",
            "       [ 0.00213534],\n",
            "       [ 0.021907  ],\n",
            "       [-0.01896361]]), array([[-0.00646917,  0.00901487,  0.02528326, -0.00248635,  0.00043669,\n",
            "        -0.00226314,  0.01331457, -0.00287308,  0.0068007 , -0.00319802],\n",
            "       [-0.01272559,  0.00313548,  0.00503185,  0.01293226, -0.00110447,\n",
            "        -0.00617362,  0.00562761,  0.00240737,  0.00280665, -0.00073113],\n",
            "       [ 0.01160339,  0.00369493,  0.01904659,  0.01111057,  0.0065905 ,\n",
            "        -0.01627438,  0.00602319,  0.00420282,  0.00810952,  0.01044442],\n",
            "       [-0.00400878,  0.00824006, -0.00562305,  0.01954878, -0.01331952,\n",
            "        -0.01760689, -0.01650721, -0.00890556, -0.01119115,  0.01956079]]), array([[-0.00326499],\n",
            "       [-0.01342676],\n",
            "       [ 0.01114383],\n",
            "       [-0.00586524]]))\n",
            "Output after one forward pass: [[0.24995552 0.24995563 0.24995495 0.24995529 0.24995578 0.24995551\n",
            "  0.2499549  0.24995447 0.24995493 0.24995574]\n",
            " [0.24737403 0.24737401 0.24737396 0.24737397 0.24737398 0.24737402\n",
            "  0.24737402 0.24737415 0.24737447 0.24737403]\n",
            " [0.25357858 0.25357866 0.25357823 0.25357862 0.25357868 0.25357884\n",
            "  0.25357812 0.25357806 0.25357822 0.25357864]\n",
            " [0.24909186 0.2490917  0.24909285 0.24909213 0.24909156 0.24909164\n",
            "  0.24909295 0.24909332 0.24909239 0.24909159]]\n",
            "Updated parameters after 1st pass -  (array([[ 0.01624349, -0.00611758, -0.0052817 , -0.01072974,  0.00865404],\n",
            "       [-0.02301539,  0.01744814, -0.00761204,  0.00319048, -0.00249368],\n",
            "       [ 0.01462108, -0.02060141, -0.00322417, -0.00384054,  0.01133769],\n",
            "       [-0.01099891, -0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
            "       [-0.01100618,  0.01144704,  0.0090158 ,  0.00502511,  0.00900836],\n",
            "       [-0.00683735, -0.00122894, -0.0093578 , -0.00267904,  0.00530347],\n",
            "       [-0.00691674, -0.00396758, -0.00687178, -0.00845206, -0.00671256],\n",
            "       [-0.00012673, -0.01117319,  0.00234422,  0.01659798,  0.00742029],\n",
            "       [-0.00191824, -0.00887622, -0.0074715 ,  0.01692471,  0.00050816],\n",
            "       [-0.00637019,  0.00190941,  0.02100256,  0.00120146,  0.00617218]]), array([[ 0.00300176],\n",
            "       [-0.0035224 ],\n",
            "       [-0.01142518],\n",
            "       [-0.00349343],\n",
            "       [-0.00208901],\n",
            "       [ 0.00586585],\n",
            "       [ 0.00838951],\n",
            "       [ 0.00931095],\n",
            "       [ 0.00285618],\n",
            "       [ 0.00885113]]), array([[-0.00754398,  0.01252868,  0.0051293 , -0.00298093,  0.00488518,\n",
            "        -0.00075572,  0.01131629,  0.01519817,  0.02185575, -0.01396496],\n",
            "       [-0.01444114, -0.00504466,  0.00160037,  0.00876169,  0.00315635,\n",
            "        -0.02022201, -0.00306204,  0.00827975,  0.00230095,  0.00762011],\n",
            "       [-0.00222353, -0.00200758,  0.00186561,  0.00410052,  0.00198325,\n",
            "         0.00119008, -0.00670665,  0.00377545,  0.00121814,  0.01129484],\n",
            "       [ 0.01198918,  0.00185156, -0.00375285, -0.0063873 ,  0.00423494,\n",
            "         0.0007734 , -0.00343854,  0.00043597, -0.00620001,  0.00698032],\n",
            "       [-0.00447136,  0.01224508,  0.00403492,  0.00593579, -0.01094889,\n",
            "         0.00169384,  0.00740554, -0.00953709, -0.00266223,  0.00032615],\n",
            "       [-0.01373117,  0.00315159,  0.00846161, -0.00859516,  0.00350546,\n",
            "        -0.01312283, -0.00038696, -0.01615772,  0.01121418,  0.00408901],\n",
            "       [-0.00024628, -0.00775162,  0.01273756,  0.01967102, -0.01857958,\n",
            "         0.01236159,  0.01627647,  0.00337976, -0.01199291,  0.00863349],\n",
            "       [-0.00180919, -0.00603921, -0.01230058,  0.00550537,  0.00792823,\n",
            "        -0.0062353 ,  0.00520575, -0.01144349,  0.00801855,  0.00046568],\n",
            "       [-0.00186578, -0.00101746,  0.00868886,  0.00750412,  0.00529487,\n",
            "         0.00137701,  0.00077819,  0.00618364,  0.00232485,  0.00682553],\n",
            "       [-0.00310117, -0.02434838,  0.01038825,  0.0218698 ,  0.00441364,\n",
            "        -0.00100155, -0.00136445, -0.00119054,  0.00017409, -0.01122019]]), array([[-0.00517094],\n",
            "       [-0.00997027],\n",
            "       [ 0.00247896],\n",
            "       [-0.00296641],\n",
            "       [ 0.0049487 ],\n",
            "       [-0.00174703],\n",
            "       [ 0.00984843],\n",
            "       [ 0.00213305],\n",
            "       [ 0.02190034],\n",
            "       [-0.01896361]]), array([[-0.00646917,  0.00901487,  0.02528199, -0.00248635,  0.00043437,\n",
            "        -0.00226314,  0.01330967, -0.00287405,  0.00678969, -0.00319802],\n",
            "       [-0.01272559,  0.00313548,  0.00503062,  0.01293226, -0.00110672,\n",
            "        -0.00617362,  0.005623  ,  0.00240643,  0.00279616, -0.00073113],\n",
            "       [ 0.01160339,  0.00369493,  0.01904781,  0.01111057,  0.00659256,\n",
            "        -0.01627438,  0.00602752,  0.00420382,  0.00811984,  0.01044442],\n",
            "       [-0.00400878,  0.00824006, -0.00562178,  0.01954878, -0.01331701,\n",
            "        -0.01760689, -0.01650202, -0.00890464, -0.01117997,  0.01956079]]), array([[-0.00376455],\n",
            "       [-0.0139005 ],\n",
            "       [ 0.01160805],\n",
            "       [-0.00535616]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tSGgxdITAUz2"
      }
    }
  ]
}